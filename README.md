# -
网络结构即训练步骤：
该全连接神经网络模型我们采用一个隐藏层，一个输出层，其中隐藏层与输出层连接一层激活函数层，我们采用Relu函数，隐藏层的神经元个数为超参数hidden_layer_size，输出层的神经元个数为10，输出层输入给softmax函数得到最终的模型预测结果.
我们先随机初始化隐藏层和输出层的网络参数w和b，优化器选择随机梯度下降法（SGD），其实训练的每一个batch_size设置为50，每一轮epoch的迭代次数即训练集的行数/bacth_size，我们采用交叉熵损失函数及l2正则化来计算每一次迭代过程中的损失，参数的更新我们采用反向传播算法，最终训练出模型。
模型参数探索：

我们一共有三个超参数供我们进行模型调参，分别为学习率learing_rate,隐藏层神经元个数hidden_layer_num，正则化强度reg_factor，为了减少调参规模，我们先初始化hidden_layer_num = 100, reg_factor = 0.01, 学习率分别选择0.1，0.01，0.001，0.0001，0.00001对模型进行训练并得到测试集预测准确率，结果如下：
 ![image](https://user-images.githubusercontent.com/83133172/162625906-e31ef245-9f3e-4b82-9198-c0cc1cf48316.png)

可以发现，i=0和i=1时即learning_rate为0.00001和0.0001时模型准确率均很高，并且在learning_rate为0.0001时，模型准确率最高，故我们将learning_rate设置为0.0001.
	接下来搜索reg_factor和hidden_layer_num的超参数值，我们分别将hidden_layer_num取50，100，500，1000，reg_factor分别取0.1，0.01，0.001，0.0001训练模型并预测测试集准确率，结果如下：
hidden_layer_num
\reg_factor	0.1	0.01	0.001	0.0001
50	0.9626	0.9517	0.9516	0.9475
100	0.9679	0.9616	0.9604	0.9566
500	0.9719	0.961	0.9642	0.962
1000	0.9723	0.9659	0.9614	0.962
可以发现，在隐藏层神经元数量为500和1000时，模型预测准确率差不多并且均达到了0.97以上，因此我们选择hidden_layer_num = 500,reg_factor = 0.1.
